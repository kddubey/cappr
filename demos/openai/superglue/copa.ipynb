{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Description**: demonstrates that the zero-shot text classification method [described\n",
                "here](https://stats.stackexchange.com/q/601159/337906) works well on the [COPA\n",
                "task](https://people.ict.usc.edu/~gordon/copa.html). It's one of the [SuperGLUE\n",
                "tasks](https://super.gluebenchmark.com/tasks) in which labels have multiple tokens, in\n",
                "some sense. An interesting result is that text generation using `text-curie-001` (a\n",
                "smaller GPT-3 model) performs worse than random guessing, while CAPPr using\n",
                "`text-curie-001` is 80% accurate. And using CAPPr with `gpt-3.5-instruct-turbo` results\n",
                "in 3% greater accuracy than using text generation with that model (see the very bottom\n",
                "of this notebook). May be SOTA for zero-shot.\n",
                "\n",
                "**Contamination notice**: I don't know whether the models used here were trained on any\n",
                "COPA data. If they were, but there's no interaction between the method (CAPPr vs text\n",
                "generation) and training, then the difference between performances can be studied.\n",
                "\n",
                "**Estimated run time**: ~1 min.\n",
                "\n",
                "**Environment**: See the [Setup section in the\n",
                "README](https://github.com/kddubey/cappr/#installation).\n",
                "\n",
                "**Other**: You have to have an OpenAI API key stored in the environment variable\n",
                "`OPENAI_API_KEY`. [Sign up here](https://openai.com/api/). This notebook will manually\n",
                "ask you to give the go-ahead before incurring any costs. Running the whole notebook will\n",
                "cost ya 30 cents.\n",
                "\n",
                "**TODO**: analyze mispredictions."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[Load data](#load-data)\n",
                "\n",
                "[Write prompt](#write-prompt)\n",
                "\n",
                "[Run model](#run-model)\n",
                "\n",
                "[Evaluate text generation](#evaluate-text-generation)\n",
                "\n",
                "[Evaluate text generation (chat)](#evaluate-text-generation-chat)\n",
                "\n",
                "[Evaluate question](#evaluate-question)\n",
                "\n",
                "[Evaluate single-token](#evaluate-single-token)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "from __future__ import annotations\n",
                "import os\n",
                "import sys\n",
                "from typing import Collection, Literal, Sequence\n",
                "\n",
                "import numpy as np\n",
                "import datasets as nlp_datasets\n",
                "import pandas as pd\n",
                "\n",
                "from cappr import Example\n",
                "from cappr import openai\n",
                "\n",
                "sys.path.insert(1, os.path.join(sys.path[0], \"..\", \"..\"))\n",
                "from utils import display_df, remove_prefix"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Load data"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "For this MVP, let's evaluate on the [Choice of Plausible Alternatives (COPA)\n",
                "task](https://people.ict.usc.edu/~gordon/copa.html). I picked this first b/c I read it\n",
                "has multi-token labels, in some sense.\n",
                "\n",
                "The classification problem is to pick 1 of 2 alternatives which caused or resulted in\n",
                "the premise. Here are two example pulled from the website:\n",
                "\n",
                "Example 1\n",
                "\n",
                "> Premise: The man broke his toe. What was the CAUSE of this?\n",
                ">\n",
                "> Alternative 1: He got a hole in his sock.\n",
                ">\n",
                "> Alternative 2: He dropped a hammer on his foot.\n",
                "\n",
                "\n",
                "Example 2\n",
                "\n",
                "> Premise: I tipped the bottle. What happened as a RESULT?\n",
                ">\n",
                "> Alternative 1: The liquid in the bottle froze.\n",
                ">\n",
                "> Alternative 2: The liquid in the bottle poured out.\n",
                "\n",
                "A classifier should predict Alternative 2 for Example 1, and Alternative 2 for Example\n",
                "2."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The test set labels are hidden, so I'll score this zero-shot classifier on the train and validation sets. We'll be evaluating 5 methods, which is quite a few for only 500 examples. But I didn't tune much of anything."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_super_glue(task_id: str, split: str):\n",
                "    return pd.DataFrame(nlp_datasets\n",
                "                        .load_dataset('super_glue', task_id, split=split))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = (pd.concat((load_super_glue('copa', 'train'),\n",
                "                 load_super_glue('copa', 'validation')))\n",
                "      .reset_index(drop=True)) # the idx column is only unique w/in splits! fuhgetaboutit"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "500"
                        ]
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "len(df)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>premise</th>\n",
                            "      <th>choice1</th>\n",
                            "      <th>choice2</th>\n",
                            "      <th>question</th>\n",
                            "      <th>idx</th>\n",
                            "      <th>label</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>My body cast a shadow over the grass.</td>\n",
                            "      <td>The sun was rising.</td>\n",
                            "      <td>The grass was cut.</td>\n",
                            "      <td>cause</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>The woman tolerated her friend's difficult beh...</td>\n",
                            "      <td>The woman knew her friend was going through a ...</td>\n",
                            "      <td>The woman felt that her friend took advantage ...</td>\n",
                            "      <td>cause</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>The women met for coffee.</td>\n",
                            "      <td>The cafe reopened in a new location.</td>\n",
                            "      <td>They wanted to catch up with each other.</td>\n",
                            "      <td>cause</td>\n",
                            "      <td>2</td>\n",
                            "      <td>1</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>The runner wore shorts.</td>\n",
                            "      <td>The forecast predicted high temperatures.</td>\n",
                            "      <td>She planned to run along the beach.</td>\n",
                            "      <td>cause</td>\n",
                            "      <td>3</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>The guests of the party hid behind the couch.</td>\n",
                            "      <td>It was a surprise party.</td>\n",
                            "      <td>It was a birthday party.</td>\n",
                            "      <td>cause</td>\n",
                            "      <td>4</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "                                             premise  \\\n",
                            "0              My body cast a shadow over the grass.   \n",
                            "1  The woman tolerated her friend's difficult beh...   \n",
                            "2                          The women met for coffee.   \n",
                            "3                            The runner wore shorts.   \n",
                            "4      The guests of the party hid behind the couch.   \n",
                            "\n",
                            "                                             choice1  \\\n",
                            "0                                The sun was rising.   \n",
                            "1  The woman knew her friend was going through a ...   \n",
                            "2               The cafe reopened in a new location.   \n",
                            "3          The forecast predicted high temperatures.   \n",
                            "4                           It was a surprise party.   \n",
                            "\n",
                            "                                             choice2 question  idx  label  \n",
                            "0                                 The grass was cut.    cause    0      0  \n",
                            "1  The woman felt that her friend took advantage ...    cause    1      0  \n",
                            "2           They wanted to catch up with each other.    cause    2      1  \n",
                            "3                She planned to run along the beach.    cause    3      0  \n",
                            "4                           It was a birthday party.    cause    4      0  "
                        ]
                    },
                    "execution_count": 6,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "df.head()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Write prompt"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "A simple way to model COPA is to prompt an LM with (for Example 1):\n",
                "\n",
                "```\n",
                "The man broke his toe because \n",
                "```\n",
                "\n",
                "and use the LM to estimate the probabilities of the 2 alternatives conditional on this\n",
                "prompt. (See the **Example** section\n",
                "[here](https://stats.stackexchange.com/q/601159/337906) for a full description of what\n",
                "\"estimate the probabilities\" actually means.)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "def _conjunction(question: Literal[\"cause\", \"effect\"]):\n",
                "    if question == \"cause\":\n",
                "        return \" because\"\n",
                "    elif question == \"effect\":\n",
                "        return \", so\"\n",
                "    else:\n",
                "        raise ValueError(\"question must be 'cause' or 'effect'. Got \" f\"{question}.\")\n",
                "\n",
                "\n",
                "def prompt(premise: str, question: Literal[\"cause\", \"effect\"]):\n",
                "    conjunction = _conjunction(question)\n",
                "    return f'{premise.strip(\". \")}{conjunction}'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "df[\"prompt\"] = [\n",
                "    prompt(premise, question)\n",
                "    for premise, question in zip(df[\"premise\"], df[\"question\"])\n",
                "]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<style type=\"text/css\">\n",
                            "#T_44e8b_row0_col0, #T_44e8b_row0_col1, #T_44e8b_row0_col2, #T_44e8b_row0_col3, #T_44e8b_row1_col0, #T_44e8b_row1_col1, #T_44e8b_row1_col2, #T_44e8b_row1_col3, #T_44e8b_row2_col0, #T_44e8b_row2_col1, #T_44e8b_row2_col2, #T_44e8b_row2_col3 {\n",
                            "  text-align: left;\n",
                            "  white-space: pre-wrap;\n",
                            "}\n",
                            "</style>\n",
                            "<table id=\"T_44e8b\">\n",
                            "  <thead>\n",
                            "    <tr>\n",
                            "      <th class=\"blank level0\" >&nbsp;</th>\n",
                            "      <th id=\"T_44e8b_level0_col0\" class=\"col_heading level0 col0\" >prompt</th>\n",
                            "      <th id=\"T_44e8b_level0_col1\" class=\"col_heading level0 col1\" >choice1</th>\n",
                            "      <th id=\"T_44e8b_level0_col2\" class=\"col_heading level0 col2\" >choice2</th>\n",
                            "      <th id=\"T_44e8b_level0_col3\" class=\"col_heading level0 col3\" >label</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th id=\"T_44e8b_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
                            "      <td id=\"T_44e8b_row0_col0\" class=\"data row0 col0\" >My body cast a shadow over the grass because</td>\n",
                            "      <td id=\"T_44e8b_row0_col1\" class=\"data row0 col1\" >The sun was rising.</td>\n",
                            "      <td id=\"T_44e8b_row0_col2\" class=\"data row0 col2\" >The grass was cut.</td>\n",
                            "      <td id=\"T_44e8b_row0_col3\" class=\"data row0 col3\" >0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th id=\"T_44e8b_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
                            "      <td id=\"T_44e8b_row1_col0\" class=\"data row1 col0\" >The woman tolerated her friend's difficult behavior because</td>\n",
                            "      <td id=\"T_44e8b_row1_col1\" class=\"data row1 col1\" >The woman knew her friend was going through a hard time.</td>\n",
                            "      <td id=\"T_44e8b_row1_col2\" class=\"data row1 col2\" >The woman felt that her friend took advantage of her kindness.</td>\n",
                            "      <td id=\"T_44e8b_row1_col3\" class=\"data row1 col3\" >0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th id=\"T_44e8b_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
                            "      <td id=\"T_44e8b_row2_col0\" class=\"data row2 col0\" >The women met for coffee because</td>\n",
                            "      <td id=\"T_44e8b_row2_col1\" class=\"data row2 col1\" >The cafe reopened in a new location.</td>\n",
                            "      <td id=\"T_44e8b_row2_col2\" class=\"data row2 col2\" >They wanted to catch up with each other.</td>\n",
                            "      <td id=\"T_44e8b_row2_col3\" class=\"data row2 col3\" >1</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n"
                        ],
                        "text/plain": [
                            "<pandas.io.formats.style.Styler at 0x11e1816d0>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "display_df(df, columns=[\"prompt\", \"choice1\", \"choice2\", \"label\"])"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Note: we need to lowercase the choices."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Run model"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Note that for many SuperGLUE datasets, including COPA, the probability distribution over\n",
                "classes (alternative 1, 2 for COPA) is uniform. So we'll use `prior=None`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "examples = [\n",
                "    Example(\n",
                "        prompt=record[\"prompt\"],\n",
                "        completions=(record[\"choice1\"].lower(), record[\"choice2\"].lower()),\n",
                "        prior=None,\n",
                "    )\n",
                "    for record in df.to_dict(\"records\")\n",
                "]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "500"
                        ]
                    },
                    "execution_count": 11,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "len(examples)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We have 500 examples * 2 classes = 1000 OpenAI API requests"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "05172b600c7440adba6da7a79da1208b",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "log-probs:   0%|          | 0/1000 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# $0.02\n",
                "pred_probs = openai.classify.predict_proba_examples(\n",
                "    examples, model=\"gpt-3.5-turbo-instruct\", ask_if_ok=True\n",
                ")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "For COPA, the scoring metric is accuracy."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "0.9"
                        ]
                    },
                    "execution_count": 13,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "(pred_probs.argmax(axis=1) == df[\"label\"]).mean()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "To put this number in context, we'll evaluate zero-shot text generation on\n",
                "`gpt-3.5-turbo-instruct`.\n",
                "\n",
                "But first, let's see how zero-shot curie performs. Curie is a much smaller, \"dumber\"\n",
                "model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "090767050aca4992a96d9fb403776392",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "log-probs:   0%|          | 0/1000 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# $0.03\n",
                "pred_probs_curie = openai.classify.predict_proba_examples(\n",
                "    examples, model=\"text-curie-001\", ask_if_ok=True\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "0.802"
                        ]
                    },
                    "execution_count": 15,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "(pred_probs_curie.argmax(axis=1) == df[\"label\"]).mean()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "TODO: diagnose these mispredictions. For example, are many caused by differing\n",
                "completion lengths? It's possible that the average likelihood metric is getting thrown\n",
                "off in those casses."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Evaluate text generation"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "COPA isn't a great demo for this approach b/c there's a trivial way to transform\n",
                "multi-token labels to single tokens: just point to each choice with a single letter!\n",
                "\n",
                "For example:\n",
                "\n",
                "```\n",
                "The man broke his toe because\n",
                "A. He got a hole in his sock.\n",
                "B. He dropped a hammer on his foot.\n",
                "Answer A or B.\n",
                "```\n",
                "\n",
                "This prompt is a multiple choice question. And it could probably work well for all of\n",
                "the SuperGLUE tasks, because they're all binary classification."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<style type=\"text/css\">\n",
                            "#T_67ec2_row0_col0, #T_67ec2_row0_col1, #T_67ec2_row1_col0, #T_67ec2_row1_col1, #T_67ec2_row2_col0, #T_67ec2_row2_col1 {\n",
                            "  text-align: left;\n",
                            "  white-space: pre-wrap;\n",
                            "}\n",
                            "</style>\n",
                            "<table id=\"T_67ec2\">\n",
                            "  <thead>\n",
                            "    <tr>\n",
                            "      <th class=\"blank level0\" >&nbsp;</th>\n",
                            "      <th id=\"T_67ec2_level0_col0\" class=\"col_heading level0 col0\" >prompt_mc</th>\n",
                            "      <th id=\"T_67ec2_level0_col1\" class=\"col_heading level0 col1\" >label</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th id=\"T_67ec2_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
                            "      <td id=\"T_67ec2_row0_col0\" class=\"data row0 col0\" >My body cast a shadow over the grass because\n",
                            "A. The sun was rising.\n",
                            "B. The grass was cut.\n",
                            "Answer A or B: </td>\n",
                            "      <td id=\"T_67ec2_row0_col1\" class=\"data row0 col1\" >0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th id=\"T_67ec2_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
                            "      <td id=\"T_67ec2_row1_col0\" class=\"data row1 col0\" >The woman tolerated her friend's difficult behavior because\n",
                            "A. The woman knew her friend was going through a hard time.\n",
                            "B. The woman felt that her friend took advantage of her kindness.\n",
                            "Answer A or B: </td>\n",
                            "      <td id=\"T_67ec2_row1_col1\" class=\"data row1 col1\" >0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th id=\"T_67ec2_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
                            "      <td id=\"T_67ec2_row2_col0\" class=\"data row2 col0\" >The women met for coffee because\n",
                            "A. The cafe reopened in a new location.\n",
                            "B. They wanted to catch up with each other.\n",
                            "Answer A or B: </td>\n",
                            "      <td id=\"T_67ec2_row2_col1\" class=\"data row2 col1\" >1</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n"
                        ],
                        "text/plain": [
                            "<pandas.io.formats.style.Styler at 0x12150d390>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "def prompt_mc(\n",
                "    premise: str, question: Literal[\"cause\", \"effect\"], choice1: str, choice2: str\n",
                "):\n",
                "    return (\n",
                "        f\"{prompt(premise, question)}\\n\"\n",
                "        f\"A. {choice1}\\n\"\n",
                "        f\"B. {choice2}\\n\"\n",
                "        \"Answer A or B: \"\n",
                "    )\n",
                "\n",
                "\n",
                "df[\"prompt_mc\"] = [\n",
                "    prompt_mc(\n",
                "        record[\"premise\"], record[\"question\"], record[\"choice1\"], record[\"choice2\"]\n",
                "    )\n",
                "    for record in df.to_dict(\"records\")\n",
                "]\n",
                "\n",
                "\n",
                "display_df(df, columns=[\"prompt_mc\", \"label\"])"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "(It turns out that GitHub doesn't render the newlines, but I promise they're there!)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "578b37443d6d49d581f87751765b219d",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "log-probs:   0%|          | 0/500 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# $0.03\n",
                "choices = openai.api.gpt_complete(\n",
                "    df[\"prompt_mc\"],\n",
                "    ask_if_ok=True,\n",
                "    model=\"gpt-3.5-turbo-instruct\",\n",
                "    max_tokens=5,  # need to allow for \"\\n\\nAnswer A\"\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": [
                "completions_mc = [choice[\"text\"] for choice in choices]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [],
            "source": [
                "def process_completion(\n",
                "    completion: str,\n",
                "    class_chars: Sequence[str],\n",
                "    prefixes_remove: Collection[str] = (\"Answer \",),\n",
                "    strip_chars: str = \" \\n.\",\n",
                "    default=-1,\n",
                ") -> int:\n",
                "    if any(len(class_char) != 1 for class_char in class_chars):\n",
                "        raise ValueError(\"Elements of class_chars must be a single character.\")\n",
                "\n",
                "    completion_stripped = completion.strip(strip_chars)\n",
                "    if not completion_stripped:\n",
                "        return default\n",
                "    for prefix_remove in prefixes_remove:\n",
                "        completion_stripped_rm = remove_prefix(completion_stripped, prefix_remove)\n",
                "    if not completion:\n",
                "        return default\n",
                "    completion_char_lower = completion_stripped_rm[0].lower()\n",
                "    class_chars_lower = [class_char.lower() for class_char in class_chars]\n",
                "    try:\n",
                "        return class_chars_lower.index(completion_char_lower)\n",
                "    except ValueError:\n",
                "        return default"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [],
            "source": [
                "class_chars = (\"A\", \"B\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [],
            "source": [
                "pred_classes_text_gen = pd.Series([\n",
                "    process_completion(completion, class_chars) for completion in completions_mc\n",
                "])"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Check that most of the sampled completions could be mapped to a label 0 or 1:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "0.99"
                        ]
                    },
                    "execution_count": 22,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "(pred_classes_text_gen != -1).mean()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Impute these with a random choice."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [],
            "source": [
                "pred_classes_text_gen_imputed = pred_classes_text_gen.copy()\n",
                "\n",
                "num_invalid = int((pred_classes_text_gen_imputed == -1).sum())\n",
                "random_preds = np.random.default_rng(90139).choice(\n",
                "    [0, 1], size=num_invalid, replace=True\n",
                ")\n",
                "pred_classes_text_gen_imputed[pred_classes_text_gen_imputed == -1] = random_preds"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "0.92"
                        ]
                    },
                    "execution_count": 24,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "(pred_classes_text_gen_imputed == df[\"label\"]).mean()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This hovers between 0.91 - 0.93 in repeated runs. A source of error is my\n",
                "`prefixes_remove` function. Let's look at all wrong predictions, and see how many are\n",
                "due to bad processing on my part:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>raw</th>\n",
                            "      <th>processed</th>\n",
                            "      <th>imputed</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>32</th>\n",
                            "      <td>\\nNeither A nor B</td>\n",
                            "      <td>None</td>\n",
                            "      <td>A</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>40</th>\n",
                            "      <td>B. His landlord repaired</td>\n",
                            "      <td>B</td>\n",
                            "      <td>B</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>68</th>\n",
                            "      <td>\\n\\nA. The owner</td>\n",
                            "      <td>A</td>\n",
                            "      <td>A</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>75</th>\n",
                            "      <td>A. He got out</td>\n",
                            "      <td>A</td>\n",
                            "      <td>A</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>82</th>\n",
                            "      <td>\\nB. He blew</td>\n",
                            "      <td>B</td>\n",
                            "      <td>B</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>86</th>\n",
                            "      <td>\\nA. I jog</td>\n",
                            "      <td>A</td>\n",
                            "      <td>A</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>104</th>\n",
                            "      <td>Either option could potentially lead</td>\n",
                            "      <td>None</td>\n",
                            "      <td>B</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>109</th>\n",
                            "      <td>A. I paid attention</td>\n",
                            "      <td>A</td>\n",
                            "      <td>A</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>117</th>\n",
                            "      <td>B. The teacher graded</td>\n",
                            "      <td>B</td>\n",
                            "      <td>B</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>128</th>\n",
                            "      <td>None of the above\\n\\n</td>\n",
                            "      <td>None</td>\n",
                            "      <td>A</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>139</th>\n",
                            "      <td>\\nA. The homeowners</td>\n",
                            "      <td>A</td>\n",
                            "      <td>A</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>142</th>\n",
                            "      <td>\\nA. The man</td>\n",
                            "      <td>A</td>\n",
                            "      <td>A</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>148</th>\n",
                            "      <td>\\nAnswer A. He</td>\n",
                            "      <td>A</td>\n",
                            "      <td>A</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>155</th>\n",
                            "      <td>\\nA. I walked</td>\n",
                            "      <td>A</td>\n",
                            "      <td>A</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>168</th>\n",
                            "      <td>\\nA. Her accomp</td>\n",
                            "      <td>A</td>\n",
                            "      <td>A</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>171</th>\n",
                            "      <td>A. The gum stuck</td>\n",
                            "      <td>A</td>\n",
                            "      <td>A</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>182</th>\n",
                            "      <td>\\nA. I water</td>\n",
                            "      <td>A</td>\n",
                            "      <td>A</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>187</th>\n",
                            "      <td>\\nA. She deleted</td>\n",
                            "      <td>A</td>\n",
                            "      <td>A</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>197</th>\n",
                            "      <td>\\nA. I lit</td>\n",
                            "      <td>A</td>\n",
                            "      <td>A</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>202</th>\n",
                            "      <td>A. He planted a</td>\n",
                            "      <td>A</td>\n",
                            "      <td>A</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>221</th>\n",
                            "      <td>A. Her parents took</td>\n",
                            "      <td>A</td>\n",
                            "      <td>A</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>251</th>\n",
                            "      <td></td>\n",
                            "      <td>None</td>\n",
                            "      <td>B</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>255</th>\n",
                            "      <td>\\nA. The traffic</td>\n",
                            "      <td>A</td>\n",
                            "      <td>A</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>263</th>\n",
                            "      <td>\\nA. He looked</td>\n",
                            "      <td>A</td>\n",
                            "      <td>A</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>266</th>\n",
                            "      <td>A. The lid on</td>\n",
                            "      <td>A</td>\n",
                            "      <td>A</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>279</th>\n",
                            "      <td>A. She reclined</td>\n",
                            "      <td>A</td>\n",
                            "      <td>A</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>285</th>\n",
                            "      <td>\\nA. I encouraged</td>\n",
                            "      <td>A</td>\n",
                            "      <td>A</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>287</th>\n",
                            "      <td>\\nA. I blew</td>\n",
                            "      <td>A</td>\n",
                            "      <td>A</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>323</th>\n",
                            "      <td>B. The pedestrian waited</td>\n",
                            "      <td>B</td>\n",
                            "      <td>B</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>341</th>\n",
                            "      <td>\\n\\nB. Her friend</td>\n",
                            "      <td>B</td>\n",
                            "      <td>B</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>364</th>\n",
                            "      <td>\\nB. She swallowed</td>\n",
                            "      <td>B</td>\n",
                            "      <td>B</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>366</th>\n",
                            "      <td>A</td>\n",
                            "      <td>A</td>\n",
                            "      <td>A</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>367</th>\n",
                            "      <td>\\nB. The spy</td>\n",
                            "      <td>B</td>\n",
                            "      <td>B</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>384</th>\n",
                            "      <td>B. The book was</td>\n",
                            "      <td>B</td>\n",
                            "      <td>B</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>421</th>\n",
                            "      <td>A</td>\n",
                            "      <td>A</td>\n",
                            "      <td>A</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>461</th>\n",
                            "      <td>B. The car radio</td>\n",
                            "      <td>B</td>\n",
                            "      <td>B</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>463</th>\n",
                            "      <td>\\nA</td>\n",
                            "      <td>A</td>\n",
                            "      <td>A</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>468</th>\n",
                            "      <td>Both A and B could</td>\n",
                            "      <td>B</td>\n",
                            "      <td>B</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>493</th>\n",
                            "      <td>1A\\n\\nA</td>\n",
                            "      <td>None</td>\n",
                            "      <td>A</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>496</th>\n",
                            "      <td>A. I was sw</td>\n",
                            "      <td>A</td>\n",
                            "      <td>A</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>498</th>\n",
                            "      <td>\\nA. I got</td>\n",
                            "      <td>A</td>\n",
                            "      <td>A</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "                                      raw processed imputed\n",
                            "32                      \\nNeither A nor B      None       A\n",
                            "40               B. His landlord repaired         B       B\n",
                            "68                       \\n\\nA. The owner         A       A\n",
                            "75                          A. He got out         A       A\n",
                            "82                           \\nB. He blew         B       B\n",
                            "86                             \\nA. I jog         A       A\n",
                            "104  Either option could potentially lead      None       B\n",
                            "109                   A. I paid attention         A       A\n",
                            "117                 B. The teacher graded         B       B\n",
                            "128                 None of the above\\n\\n      None       A\n",
                            "139                   \\nA. The homeowners         A       A\n",
                            "142                          \\nA. The man         A       A\n",
                            "148                        \\nAnswer A. He         A       A\n",
                            "155                         \\nA. I walked         A       A\n",
                            "168                       \\nA. Her accomp         A       A\n",
                            "171                      A. The gum stuck         A       A\n",
                            "182                          \\nA. I water         A       A\n",
                            "187                      \\nA. She deleted         A       A\n",
                            "197                            \\nA. I lit         A       A\n",
                            "202                       A. He planted a         A       A\n",
                            "221                   A. Her parents took         A       A\n",
                            "251                                            None       B\n",
                            "255                      \\nA. The traffic         A       A\n",
                            "263                        \\nA. He looked         A       A\n",
                            "266                         A. The lid on         A       A\n",
                            "279                       A. She reclined         A       A\n",
                            "285                     \\nA. I encouraged         A       A\n",
                            "287                           \\nA. I blew         A       A\n",
                            "323              B. The pedestrian waited         B       B\n",
                            "341                     \\n\\nB. Her friend         B       B\n",
                            "364                    \\nB. She swallowed         B       B\n",
                            "366                                     A         A       A\n",
                            "367                          \\nB. The spy         B       B\n",
                            "384                       B. The book was         B       B\n",
                            "421                                     A         A       A\n",
                            "461                      B. The car radio         B       B\n",
                            "463                                   \\nA         A       A\n",
                            "468                    Both A and B could         B       B\n",
                            "493                               1A\\n\\nA      None       A\n",
                            "496                           A. I was sw         A       A\n",
                            "498                            \\nA. I got         A       A"
                        ]
                    },
                    "execution_count": 25,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "mask_wrong = pred_classes_text_gen != df[\"label\"]\n",
                "\n",
                "\n",
                "def to_letter(class_idx: int) -> str:\n",
                "    if class_idx < 0:\n",
                "        return None\n",
                "    return class_chars[class_idx]\n",
                "\n",
                "\n",
                "pd.DataFrame(\n",
                "    {\n",
                "        \"raw\": pd.Series(completions_mc)[mask_wrong],\n",
                "        \"processed\": pred_classes_text_gen[mask_wrong].apply(to_letter),\n",
                "        \"imputed\": pred_classes_text_gen_imputed[mask_wrong].apply(to_letter),\n",
                "    }\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Looks like I missed a small number. Maybe accuracy is 0.5% higher with perfect\n",
                "post-processing. All of the `\"I don't know\"` responses are wrong."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's see how text generation w/ `text-curie-001` performs. Hypothesis: shouldn't be too\n",
                "bad given the curie result above."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "3dbc8c77ee4b4f0595dea9d5ebbbbd7b",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "log-probs:   0%|          | 0/500 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# $0.04\n",
                "choices_curie = openai.api.gpt_complete(\n",
                "    df[\"prompt_mc\"], ask_if_ok=True, model=\"text-curie-001\", max_tokens=5\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [],
            "source": [
                "completions_mc_curie = [choice[\"text\"] for choice in choices_curie]\n",
                "pred_classes_text_gen_curie = [\n",
                "    process_completion(completion, class_chars) for completion in completions_mc_curie\n",
                "]"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's see how many of these sampled completions are actually \"valid\", i.e., in the label set"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "0.802"
                        ]
                    },
                    "execution_count": 28,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "pred_classes_text_gen_curie = pd.Series(pred_classes_text_gen_curie, index=df.index)\n",
                "(pred_classes_text_gen_curie != -1).mean()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "0.434"
                        ]
                    },
                    "execution_count": 29,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "(pred_classes_text_gen_curie == df[\"label\"]).mean()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Ouch, much worse than random guessing. Hypothesis very rejected. Let's see how often the\n",
                "valid completions are accurate."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "0.5411471321695761"
                        ]
                    },
                    "execution_count": 30,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "_mask_valid = pred_classes_text_gen_curie != -1\n",
                "(pred_classes_text_gen_curie[_mask_valid] == df.loc[_mask_valid, \"label\"]).mean()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Evaluate text generation (chat)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "How does the chat completion endpoint perform on COPA? I think it makes sense to use the\n",
                "same prompt as above."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [],
            "source": [
                "system_prompt_copa = (\n",
                "    \"Identify the cause or effect of a premise given two choices. Each choice \"\n",
                "    \"is identified by a letter, A or B.\\n\"\n",
                "    \"Respond only with the letter corresponding to the correct cause or effect.\"\n",
                ")\n",
                "# getting this right is crucial"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Sometimes I need to re-run this next cell, b/c the API doesn't like me:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "d09b868be6e44e008cbabe18d0596e5f",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Completing chats:   0%|          | 0/500 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# $0.04\n",
                "# can take a while, 5 minutes!\n",
                "# idk and idc yet how to batch for the chat endpoint. For correctness, I'll just send\n",
                "# texts 1-by-1\n",
                "choices_chat = openai.api.gpt_chat_complete(\n",
                "    df[\"prompt_mc\"], ask_if_ok=True, max_tokens=5, system_msg=system_prompt_copa\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [],
            "source": [
                "completions_chat = pd.Series(\n",
                "    [choice[\"message\"][\"content\"] for choice in choices_chat], index=df.index\n",
                ")\n",
                "\n",
                "pred_classes_chat = pd.Series(\n",
                "    [process_completion(completion, class_chars) for completion in completions_chat],\n",
                "    index=df.index,\n",
                ")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "As usual, we need to check that completions are valid."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "1.0"
                        ]
                    },
                    "execution_count": 34,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "mask_valid = pred_classes_chat != -1\n",
                "mask_valid.mean()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "What do invalid completions look like?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "Series([], dtype: object)"
                        ]
                    },
                    "execution_count": 35,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "completions_chat[~mask_valid]"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "What's the accuracy on all completions?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "0.898"
                        ]
                    },
                    "execution_count": 36,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "(pred_classes_chat == df[\"label\"]).mean()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "What's the accuracy on *valid* completions?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "0.898"
                        ]
                    },
                    "execution_count": 37,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "(pred_classes_chat[mask_valid] == df.loc[mask_valid, \"label\"]).mean()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Hovers around 0.895-0.915 in repeated runs."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Evaluate question"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "There are different ways to format a prompt-completion problem. Since `gpt-3.5-turbo-instruct` was trained w/ RLHF, it's worth asking whether a more RLHF-type of prompt would work better. Let's see how performance changes by formatting the problem as a question:\n",
                "\n",
                "```\n",
                "The man broke his toe. What was the cause of this? \n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<style type=\"text/css\">\n",
                            "#T_e233c_row0_col0, #T_e233c_row0_col1, #T_e233c_row0_col2, #T_e233c_row0_col3, #T_e233c_row1_col0, #T_e233c_row1_col1, #T_e233c_row1_col2, #T_e233c_row1_col3, #T_e233c_row2_col0, #T_e233c_row2_col1, #T_e233c_row2_col2, #T_e233c_row2_col3 {\n",
                            "  text-align: left;\n",
                            "  white-space: pre-wrap;\n",
                            "}\n",
                            "</style>\n",
                            "<table id=\"T_e233c\">\n",
                            "  <thead>\n",
                            "    <tr>\n",
                            "      <th class=\"blank level0\" >&nbsp;</th>\n",
                            "      <th id=\"T_e233c_level0_col0\" class=\"col_heading level0 col0\" >prompt_question</th>\n",
                            "      <th id=\"T_e233c_level0_col1\" class=\"col_heading level0 col1\" >choice1</th>\n",
                            "      <th id=\"T_e233c_level0_col2\" class=\"col_heading level0 col2\" >choice2</th>\n",
                            "      <th id=\"T_e233c_level0_col3\" class=\"col_heading level0 col3\" >label</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th id=\"T_e233c_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
                            "      <td id=\"T_e233c_row0_col0\" class=\"data row0 col0\" >My body cast a shadow over the grass. What was the cause of this?</td>\n",
                            "      <td id=\"T_e233c_row0_col1\" class=\"data row0 col1\" >The sun was rising.</td>\n",
                            "      <td id=\"T_e233c_row0_col2\" class=\"data row0 col2\" >The grass was cut.</td>\n",
                            "      <td id=\"T_e233c_row0_col3\" class=\"data row0 col3\" >0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th id=\"T_e233c_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
                            "      <td id=\"T_e233c_row1_col0\" class=\"data row1 col0\" >The woman tolerated her friend's difficult behavior. What was the cause of this?</td>\n",
                            "      <td id=\"T_e233c_row1_col1\" class=\"data row1 col1\" >The woman knew her friend was going through a hard time.</td>\n",
                            "      <td id=\"T_e233c_row1_col2\" class=\"data row1 col2\" >The woman felt that her friend took advantage of her kindness.</td>\n",
                            "      <td id=\"T_e233c_row1_col3\" class=\"data row1 col3\" >0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th id=\"T_e233c_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
                            "      <td id=\"T_e233c_row2_col0\" class=\"data row2 col0\" >The women met for coffee. What was the cause of this?</td>\n",
                            "      <td id=\"T_e233c_row2_col1\" class=\"data row2 col1\" >The cafe reopened in a new location.</td>\n",
                            "      <td id=\"T_e233c_row2_col2\" class=\"data row2 col2\" >They wanted to catch up with each other.</td>\n",
                            "      <td id=\"T_e233c_row2_col3\" class=\"data row2 col3\" >1</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n"
                        ],
                        "text/plain": [
                            "<pandas.io.formats.style.Styler at 0x12fd38890>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "def prompt_question(premise: str, question: Literal[\"cause\", \"effect\"]):\n",
                "    if question == \"cause\":\n",
                "        question_ = \"What was the cause of this?\"\n",
                "    elif question == \"effect\":\n",
                "        question_ = \"What happened as a result?\"\n",
                "    else:\n",
                "        raise ValueError(\"question must be 'cause' or 'effect'. Got \" f\"{question}.\")\n",
                "    return f\"{premise} {question_}\"\n",
                "\n",
                "\n",
                "df[\"prompt_question\"] = [\n",
                "    prompt_question(premise, question)\n",
                "    for premise, question in zip(df[\"premise\"], df[\"question\"])\n",
                "]\n",
                "\n",
                "\n",
                "display_df(df, columns=[\"prompt_question\", \"choice1\", \"choice2\", \"label\"])"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "According to the [docs](https://platform.openai.com/docs/guides/fine-tuning/data-formatting), best practice is to separate prompts and completions using this string:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'\\n\\n###\\n\\n'"
                        ]
                    },
                    "execution_count": 39,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "end_of_prompt_pseudo = '\\n\\n###\\n\\n'\n",
                "end_of_prompt_pseudo"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {},
            "outputs": [],
            "source": [
                "examples_question = [\n",
                "    Example(\n",
                "        prompt=record[\"prompt_question\"] + end_of_prompt_pseudo,\n",
                "        completions=(record[\"choice1\"], record[\"choice2\"]),\n",
                "        prior=None,\n",
                "    )\n",
                "    for record in df.to_dict(\"records\")\n",
                "]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "f8798cb8c616473981b190baeb5888a3",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "log-probs:   0%|          | 0/1000 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# $0.03\n",
                "pred_probs_question = openai.classify.predict_proba_examples(\n",
                "    examples_question, model=\"gpt-3.5-turbo-instruct\", ask_if_ok=True\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "0.82"
                        ]
                    },
                    "execution_count": 42,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "(pred_probs_question.argmax(axis=1) == df[\"label\"]).mean()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Bad! Prompt engineering is important."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Evaluate single-token"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's see how the single-token transformation performs for COPA. Based on the [Evaluate\n",
                "text generation](#evaluate-text-generation) result, my hypothesis is that it'll perform\n",
                "slightly better than the [multi-token approach](#run-model). I wouldn't be bummed if it\n",
                "performed better. B/c if I could control the backend, there's still a usability and\n",
                "computational benefit to the idea of returning probabilities for A and B instead of\n",
                "sampling from all possible token sequences."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "metadata": {},
            "outputs": [],
            "source": [
                "examples_mc = [\n",
                "    Example(\n",
                "        prompt=record[\"prompt_mc\"],\n",
                "        completions=(\"A\", \"B\"),\n",
                "        prior=None,\n",
                "        end_of_prompt=\"\",\n",
                "    )\n",
                "    for record in df.to_dict(\"records\")\n",
                "]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "1848795fb2bf4057926307845eb39ddd",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "log-probs:   0%|          | 0/1000 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# $0.05\n",
                "# If I could control the backend, the cost would be $0.05/2 = $0.025\n",
                "pred_probs_mc = openai.classify.predict_proba_examples(\n",
                "    examples_mc, model=\"gpt-3.5-turbo-instruct\", ask_if_ok=True\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "It costs twice as much. But that's an artifact of the way the endpoint works. For\n",
                "prompts like this, it takes 1 `model()` call to give us the data we need: the\n",
                "probability distribution of (single tokens) `'A'` and `'B'` conditional on the prompt."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 45,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "0.95"
                        ]
                    },
                    "execution_count": 45,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "(pred_probs_mc.argmax(axis=1) == df[\"label\"]).mean()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The gap between:\n",
                "\n",
                "- asking to generate a single token w/ `temperature=0` (93% accuracy)\n",
                "- CAPPr where completions are single tokens (95% accuracy).\n",
                "\n",
                "is explained, in most part, by the LLM incorectly generating `\"I don't know\"`,\n",
                "`\"Neither\"`, and `\"Both\"`. Another explanatation is the use of `<|endoftext|>` when\n",
                "generating text. But I thought that'd help not hurt, b/c it's used during instruction\n",
                "training."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Test"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "It's important to do some things honorably."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<span style=\"font-family: Baskerville; font-size: 18px;\">I solemnly swear that I\n",
                "evaluated on the test set once, running only the following cells in sequence\n",
                "once.</span>\n",
                "\n",
                "<img src=\"../../signature.png\" alt=\"drawing\" width=\"200\"/>\n",
                "<div style=\"width:200px\"><hr/></div>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 46,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "500"
                        ]
                    },
                    "execution_count": 46,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "df_test = load_super_glue('copa', 'test').reset_index(drop=True)\n",
                "len(df_test)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 47,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<style type=\"text/css\">\n",
                            "#T_7e727_row0_col0, #T_7e727_row0_col1, #T_7e727_row1_col0, #T_7e727_row1_col1, #T_7e727_row2_col0, #T_7e727_row2_col1 {\n",
                            "  text-align: left;\n",
                            "  white-space: pre-wrap;\n",
                            "}\n",
                            "</style>\n",
                            "<table id=\"T_7e727\">\n",
                            "  <thead>\n",
                            "    <tr>\n",
                            "      <th class=\"blank level0\" >&nbsp;</th>\n",
                            "      <th id=\"T_7e727_level0_col0\" class=\"col_heading level0 col0\" >prompt_mc</th>\n",
                            "      <th id=\"T_7e727_level0_col1\" class=\"col_heading level0 col1\" >label</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th id=\"T_7e727_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
                            "      <td id=\"T_7e727_row0_col0\" class=\"data row0 col0\" >The item was packaged in bubble wrap because\n",
                            "A. It was fragile.\n",
                            "B. It was small.\n",
                            "Answer A or B: </td>\n",
                            "      <td id=\"T_7e727_row0_col1\" class=\"data row0 col1\" >-1</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th id=\"T_7e727_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
                            "      <td id=\"T_7e727_row1_col0\" class=\"data row1 col0\" >I emptied my pockets, so\n",
                            "A. I retrieved a ticket stub.\n",
                            "B. I found a weapon.\n",
                            "Answer A or B: </td>\n",
                            "      <td id=\"T_7e727_row1_col1\" class=\"data row1 col1\" >-1</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th id=\"T_7e727_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
                            "      <td id=\"T_7e727_row2_col0\" class=\"data row2 col0\" >Termites invaded the house, so\n",
                            "A. The termites disappeared from the house.\n",
                            "B. The termites ate through the wood in the house.\n",
                            "Answer A or B: </td>\n",
                            "      <td id=\"T_7e727_row2_col1\" class=\"data row2 col1\" >-1</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n"
                        ],
                        "text/plain": [
                            "<pandas.io.formats.style.Styler at 0x122213810>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "df_test[\"prompt\"] = [\n",
                "    prompt(premise, question)\n",
                "    for premise, question in zip(df_test[\"premise\"], df_test[\"question\"])\n",
                "]\n",
                "\n",
                "df_test[\"prompt_mc\"] = [\n",
                "    prompt_mc(\n",
                "        record[\"premise\"], record[\"question\"], record[\"choice1\"], record[\"choice2\"]\n",
                "    )\n",
                "    for record in df_test.to_dict(\"records\")\n",
                "]\n",
                "\n",
                "display_df(df_test, columns=[\"prompt_mc\", \"label\"])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 48,
            "metadata": {},
            "outputs": [],
            "source": [
                "examples_mc_test = [\n",
                "    Example(\n",
                "        prompt=record[\"prompt_mc\"],\n",
                "        completions=(\"A\", \"B\"),\n",
                "        prior=None,\n",
                "        end_of_prompt=\"\",\n",
                "    )\n",
                "    for record in df_test.to_dict(\"records\")\n",
                "]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Download the test set labels from the unzipped `COPA-resources.tgz`\n",
                "[here](https://people.ict.usc.edu/~gordon/copa.html)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 49,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "f5e4fd7bc86d4422a120f2d25dab4857",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "log-probs:   0%|          | 0/1000 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# $0.05\n",
                "# If I could control the backend, the cost would be $0.05/2 = $0.025\n",
                "pred_probs_mc_test = openai.classify.predict_proba_examples(\n",
                "    examples_mc_test, model=\"gpt-3.5-turbo-instruct\", ask_if_ok=True\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 50,
            "metadata": {},
            "outputs": [],
            "source": [
                "np.save('copa_test_set_pred_probs', pred_probs_mc_test)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 51,
            "metadata": {},
            "outputs": [],
            "source": [
                "_df_test_labels = pd.read_csv(\n",
                "    \"../COPA-resources/results/gold.test\", sep=\" \", header=None\n",
                ")\n",
                "test_labels = pd.Series(\n",
                "    [int(record[1] != 1) for record in _df_test_labels.to_dict(\"records\")]\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 52,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "0.964"
                        ]
                    },
                    "execution_count": 52,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "(pred_probs_mc_test.argmax(axis=1) == test_labels).mean()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "schoolrecordschoolrecordschoolrecord"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "lmc",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.5"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "68daa88f78f5c448099edb3a6d3dee27486a6add8824ae1cbe4c903ef8faec70"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
