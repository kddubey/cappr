{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Description**: demonstrates that the zero-shot text classification method [described here](https://stats.stackexchange.com/q/601159/337906) works ok on the [COPA task](https://people.ict.usc.edu/~gordon/copa.html). It's one of the [SuperGLUE tasks](https://super.gluebenchmark.com/tasks) in which labels have multiple tokens, in some sense. An interesting result is that classification-via-sampling using `text-curie-001` (a smaller GPT-3 model) performs worse than random guessing, while CAPPr using `text-curie-001` is 80% accurate.\n",
                "\n",
                "**Contamination notice**: I don't know whether the models used here were trained on any COPA data. If they were, but there's no interaction between the method (CAPPr vs CVS) and training, then the difference between performances can be studied.\n",
                "\n",
                "**Estimated run time**: ~1 min.\n",
                "\n",
                "**Environment**: See the [Setup section in the README](https://github.com/kddubey/cappr/#setup).\n",
                "\n",
                "**Other**: You have to have an OpenAI API key stored in the environment variable\n",
                "`OPENAI_API_KEY`. [Sign up here](https://openai.com/api/). This notebook will manually\n",
                "ask you to give the go-ahead before incurring any costs. Running the whole notebook will\n",
                "cost ya north of\n",
                "<span>$</span>2!\n",
                "\n",
                "**TODO**: analyze mispredictions."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[Load data](#load-data)\n",
                "\n",
                "[Write prompt](#write-prompt)\n",
                "\n",
                "[Run model](#run-model)\n",
                "\n",
                "[Evaluate CVS](#evaluate-cvs)\n",
                "\n",
                "[Evaluate CVS (chat)](#evaluate-cvs-chat)\n",
                "\n",
                "[Evaluate question](#evaluate-question)\n",
                "\n",
                "[Evaluate single-token](#evaluate-single-token)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "from __future__ import annotations\n",
                "import logging\n",
                "import os\n",
                "import sys\n",
                "from typing import Literal, Sequence\n",
                "\n",
                "import datasets as nlp_datasets\n",
                "import pandas as pd\n",
                "\n",
                "from cappr import Example\n",
                "from cappr import openai\n",
                "sys.path.insert(1, os.path.join(sys.path[0], \"..\"))\n",
                "from utils import display_df, remove_prefix"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "## When hitting the OpenAI endpoints, we'll log any server errors\n",
                "logging.basicConfig(level=logging.INFO,\n",
                "                    handlers=[logging.StreamHandler(stream=sys.stdout)],\n",
                "                    format='%(asctime)s :: %(name)s :: %(levelname)s :: '\n",
                "                           '%(message)s')\n",
                "logger = logging.getLogger(__name__)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Load data"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "For this MVP, let's evaluate on the [Choice of Plausible Alternatives (COPA) task](https://people.ict.usc.edu/~gordon/copa.html). I picked this first b/c I read it has multi-token labels, in some sense. It also looks cool.\n",
                "\n",
                "The classification problem is to pick 1 of 2 alternatives which caused or resulted in the premise. Here are two example pulled from the website:\n",
                "\n",
                "Example 1\n",
                "\n",
                "> Premise: The man broke his toe. What was the CAUSE of this?\n",
                ">\n",
                "> Alternative 1: He got a hole in his sock.\n",
                ">\n",
                "> Alternative 2: He dropped a hammer on his foot.\n",
                "\n",
                "\n",
                "Example 2\n",
                "\n",
                "> Premise: I tipped the bottle. What happened as a RESULT?\n",
                ">\n",
                "> Alternative 1: The liquid in the bottle froze.\n",
                ">\n",
                "> Alternative 2: The liquid in the bottle poured out.\n",
                "\n",
                "A classifier should predict Alternative 2 for Example 1, and Alternative 2 for Example 2."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The test set labels are hidden, so I'll score this zero-shot classifier on the train and validation sets. We'll be evaluating 5 methods, which is quite a few for only 500 examples. But I didn't tune much of anything."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "2023-03-18 03:22:20,973 :: datasets.builder :: WARNING :: Found cached dataset super_glue (C:/Users/kushd/.cache/huggingface/datasets/super_glue/copa/1.0.3/bb9675f958ebfee0d5d6dc5476fafe38c79123727a7258d515c450873dbdbbed)\n",
                        "2023-03-18 03:22:26,941 :: datasets.builder :: WARNING :: Found cached dataset super_glue (C:/Users/kushd/.cache/huggingface/datasets/super_glue/copa/1.0.3/bb9675f958ebfee0d5d6dc5476fafe38c79123727a7258d515c450873dbdbbed)\n"
                    ]
                }
            ],
            "source": [
                "def load_super_glue(task_id: str, split: str):\n",
                "    return pd.DataFrame(nlp_datasets\n",
                "                        .load_dataset('super_glue', task_id, split=split))\n",
                "\n",
                "\n",
                "## takes about 12 seconds, sorry\n",
                "df = (pd.concat((load_super_glue('copa', 'train'),\n",
                "                 load_super_glue('copa', 'validation')))\n",
                "      .reset_index(drop=True)) ## the idx column is only unique w/in splits! fuhgetaboutit"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "500"
                        ]
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "len(df)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>premise</th>\n",
                            "      <th>choice1</th>\n",
                            "      <th>choice2</th>\n",
                            "      <th>question</th>\n",
                            "      <th>idx</th>\n",
                            "      <th>label</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>My body cast a shadow over the grass.</td>\n",
                            "      <td>The sun was rising.</td>\n",
                            "      <td>The grass was cut.</td>\n",
                            "      <td>cause</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>The woman tolerated her friend's difficult beh...</td>\n",
                            "      <td>The woman knew her friend was going through a ...</td>\n",
                            "      <td>The woman felt that her friend took advantage ...</td>\n",
                            "      <td>cause</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>The women met for coffee.</td>\n",
                            "      <td>The cafe reopened in a new location.</td>\n",
                            "      <td>They wanted to catch up with each other.</td>\n",
                            "      <td>cause</td>\n",
                            "      <td>2</td>\n",
                            "      <td>1</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>The runner wore shorts.</td>\n",
                            "      <td>The forecast predicted high temperatures.</td>\n",
                            "      <td>She planned to run along the beach.</td>\n",
                            "      <td>cause</td>\n",
                            "      <td>3</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>The guests of the party hid behind the couch.</td>\n",
                            "      <td>It was a surprise party.</td>\n",
                            "      <td>It was a birthday party.</td>\n",
                            "      <td>cause</td>\n",
                            "      <td>4</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "                                             premise  \\\n",
                            "0              My body cast a shadow over the grass.   \n",
                            "1  The woman tolerated her friend's difficult beh...   \n",
                            "2                          The women met for coffee.   \n",
                            "3                            The runner wore shorts.   \n",
                            "4      The guests of the party hid behind the couch.   \n",
                            "\n",
                            "                                             choice1  \\\n",
                            "0                                The sun was rising.   \n",
                            "1  The woman knew her friend was going through a ...   \n",
                            "2               The cafe reopened in a new location.   \n",
                            "3          The forecast predicted high temperatures.   \n",
                            "4                           It was a surprise party.   \n",
                            "\n",
                            "                                             choice2 question  idx  label  \n",
                            "0                                 The grass was cut.    cause    0      0  \n",
                            "1  The woman felt that her friend took advantage ...    cause    1      0  \n",
                            "2           They wanted to catch up with each other.    cause    2      1  \n",
                            "3                She planned to run along the beach.    cause    3      0  \n",
                            "4                           It was a birthday party.    cause    4      0  "
                        ]
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "df.head()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Write prompt"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "A simple way to model COPA is to prompt an LM with (for Example 1):\n",
                "\n",
                "```\n",
                "The man broke his toe because \n",
                "```\n",
                "\n",
                "and use the LM to estimate the probabilities of the 2 alternatives conditional on this prompt. (See the **Example** section [here](https://stats.stackexchange.com/q/601159/337906) for a full description of what \"estimate the probabilities\" actually means.)\n",
                "\n",
                "This method assumes GPT isn't miscalibrated in bad ways, as it relies entirely on the comparison between averaged probabilities. Another potential issue is that any 2 alternatives are gonna have really low probabilities. As a result, discriminating between alternatives may be, numerically and statistically, a bad idea. But that's why this notebook is here: let's see if these issues significantly impact accuracy when compared to classification via sampling (CVS). And even if they do, we could always provide the alternatives in the prompt, as would be done w/ a sampling approach. That's done in [Evaluate single-token](#evaluate-single-token)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "def _conjunction(question: Literal['cause', 'effect']):\n",
                "    if question == 'cause':\n",
                "        return ' because'\n",
                "    elif question == 'effect':\n",
                "        return ', so'\n",
                "    else:\n",
                "        raise ValueError( \"question must be 'cause' or 'effect'. Got \"\n",
                "                         f'{question}.')\n",
                "\n",
                "\n",
                "def prompt(premise: str, question: Literal['cause', 'effect']):\n",
                "    conjunction = _conjunction(question)\n",
                "    return f'{premise.strip(\". \")}{conjunction}'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "df['prompt'] = [prompt(premise, question)\n",
                "                for premise, question\n",
                "                in zip(df['premise'], df['question'])]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<style type=\"text/css\">\n",
                            "#T_20338_row0_col0, #T_20338_row0_col1, #T_20338_row0_col2, #T_20338_row0_col3, #T_20338_row1_col0, #T_20338_row1_col1, #T_20338_row1_col2, #T_20338_row1_col3, #T_20338_row2_col0, #T_20338_row2_col1, #T_20338_row2_col2, #T_20338_row2_col3 {\n",
                            "  text-align: left;\n",
                            "  white-space: pre-wrap;\n",
                            "}\n",
                            "</style>\n",
                            "<table id=\"T_20338\">\n",
                            "  <thead>\n",
                            "    <tr>\n",
                            "      <th class=\"blank level0\" >&nbsp;</th>\n",
                            "      <th id=\"T_20338_level0_col0\" class=\"col_heading level0 col0\" >prompt</th>\n",
                            "      <th id=\"T_20338_level0_col1\" class=\"col_heading level0 col1\" >choice1</th>\n",
                            "      <th id=\"T_20338_level0_col2\" class=\"col_heading level0 col2\" >choice2</th>\n",
                            "      <th id=\"T_20338_level0_col3\" class=\"col_heading level0 col3\" >label</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th id=\"T_20338_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
                            "      <td id=\"T_20338_row0_col0\" class=\"data row0 col0\" >My body cast a shadow over the grass because</td>\n",
                            "      <td id=\"T_20338_row0_col1\" class=\"data row0 col1\" >The sun was rising.</td>\n",
                            "      <td id=\"T_20338_row0_col2\" class=\"data row0 col2\" >The grass was cut.</td>\n",
                            "      <td id=\"T_20338_row0_col3\" class=\"data row0 col3\" >0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th id=\"T_20338_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
                            "      <td id=\"T_20338_row1_col0\" class=\"data row1 col0\" >The woman tolerated her friend's difficult behavior because</td>\n",
                            "      <td id=\"T_20338_row1_col1\" class=\"data row1 col1\" >The woman knew her friend was going through a hard time.</td>\n",
                            "      <td id=\"T_20338_row1_col2\" class=\"data row1 col2\" >The woman felt that her friend took advantage of her kindness.</td>\n",
                            "      <td id=\"T_20338_row1_col3\" class=\"data row1 col3\" >0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th id=\"T_20338_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
                            "      <td id=\"T_20338_row2_col0\" class=\"data row2 col0\" >The women met for coffee because</td>\n",
                            "      <td id=\"T_20338_row2_col1\" class=\"data row2 col1\" >The cafe reopened in a new location.</td>\n",
                            "      <td id=\"T_20338_row2_col2\" class=\"data row2 col2\" >They wanted to catch up with each other.</td>\n",
                            "      <td id=\"T_20338_row2_col3\" class=\"data row2 col3\" >1</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n"
                        ],
                        "text/plain": [
                            "<pandas.io.formats.style.Styler at 0x1c0c8585b80>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "display_df(df, columns=['prompt', 'choice1', 'choice2', 'label'])"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Note: we need to lowercase the choices."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Run model"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Note that for many SuperGLUE datasets, including COPA, the probability distribution over classes (alternative 1, 2 for COPA) is uniform. So we'll use `prior=None`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "examples = [Example(prompt=record['prompt'],\n",
                "                    completions=(record['choice1'].lower(),\n",
                "                                 record['choice2'].lower()),\n",
                "                    prior=None)\n",
                "            for record in df.to_dict('records')]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "500"
                        ]
                    },
                    "execution_count": 10,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "len(examples)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We have 500 examples * 2 classes = 1000 OpenAI API requests"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "07dbfed1657a4396be9422fd0b98d27d",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Computing probs:   0%|          | 0/1000 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "## $0.29\n",
                "pred_probs = (openai.classify\n",
                "              .predict_proba_examples(examples,\n",
                "                                      model='text-davinci-003',\n",
                "                                      ask_if_ok=True))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "For COPA, the scoring metric is accuracy."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "0.92"
                        ]
                    },
                    "execution_count": 12,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "(pred_probs.argmax(axis=1) == df['label']).mean()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "To put this number in context, we'll evaluate zero-shot classification via sampling (CVS) on `text-davinci-003`.\n",
                "\n",
                "But first, let's see how zero-shot curie performs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "d21a22e8bec94e9ca517f6c1be9c771f",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Computing probs:   0%|          | 0/1000 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "## $0.03\n",
                "pred_probs_curie = (openai.classify\n",
                "                    .predict_proba_examples(examples,\n",
                "                                            model='text-curie-001',\n",
                "                                            ask_if_ok=True))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "0.8"
                        ]
                    },
                    "execution_count": 14,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "(pred_probs_curie.argmax(axis=1) == df['label']).mean()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "TODO: diagnose these mispredictions. For example, are many caused by differing completion lengths? It's possible that the average likelihood metric is getting thrown off in those casses."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Evaluate CVS"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "COPA isn't a great demo for this approach b/c there's a trivial way to transform multi-token labels to single tokens: just point to each choice with a single letter!\n",
                "\n",
                "For example:\n",
                "\n",
                "```\n",
                "The man broke his toe because\n",
                "A. He got a hole in his sock.\n",
                "B. He dropped a hammer on his foot.\n",
                "Answer A or B.\n",
                "```\n",
                "\n",
                "This prompt is a multiple choice question. And it could probably work well for all of the SuperGLUE tasks, because they're all binary classification."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<style type=\"text/css\">\n",
                            "#T_76d73_row0_col0, #T_76d73_row0_col1, #T_76d73_row1_col0, #T_76d73_row1_col1, #T_76d73_row2_col0, #T_76d73_row2_col1 {\n",
                            "  text-align: left;\n",
                            "  white-space: pre-wrap;\n",
                            "}\n",
                            "</style>\n",
                            "<table id=\"T_76d73\">\n",
                            "  <thead>\n",
                            "    <tr>\n",
                            "      <th class=\"blank level0\" >&nbsp;</th>\n",
                            "      <th id=\"T_76d73_level0_col0\" class=\"col_heading level0 col0\" >prompt_mc</th>\n",
                            "      <th id=\"T_76d73_level0_col1\" class=\"col_heading level0 col1\" >label</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th id=\"T_76d73_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
                            "      <td id=\"T_76d73_row0_col0\" class=\"data row0 col0\" >My body cast a shadow over the grass because\n",
                            "A. The sun was rising.\n",
                            "B. The grass was cut.\n",
                            "Answer A or B.</td>\n",
                            "      <td id=\"T_76d73_row0_col1\" class=\"data row0 col1\" >0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th id=\"T_76d73_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
                            "      <td id=\"T_76d73_row1_col0\" class=\"data row1 col0\" >The woman tolerated her friend's difficult behavior because\n",
                            "A. The woman knew her friend was going through a hard time.\n",
                            "B. The woman felt that her friend took advantage of her kindness.\n",
                            "Answer A or B.</td>\n",
                            "      <td id=\"T_76d73_row1_col1\" class=\"data row1 col1\" >0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th id=\"T_76d73_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
                            "      <td id=\"T_76d73_row2_col0\" class=\"data row2 col0\" >The women met for coffee because\n",
                            "A. The cafe reopened in a new location.\n",
                            "B. They wanted to catch up with each other.\n",
                            "Answer A or B.</td>\n",
                            "      <td id=\"T_76d73_row2_col1\" class=\"data row2 col1\" >1</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n"
                        ],
                        "text/plain": [
                            "<pandas.io.formats.style.Styler at 0x1c0c8786d90>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "def prompt_mc(premise: str, question: Literal['cause', 'effect'],\n",
                "              choice1: str, choice2: str):\n",
                "    return (f'{prompt(premise, question)}\\n'\n",
                "            f'A. {choice1}\\n'\n",
                "            f'B. {choice2}\\n'\n",
                "             'Answer A or B.')\n",
                "\n",
                "\n",
                "df['prompt_mc'] = [prompt_mc(record['premise'], record['question'],\n",
                "                             record['choice1'], record['choice2'])\n",
                "                   for record in df.to_dict('records')]\n",
                "\n",
                "\n",
                "display_df(df, columns=['prompt_mc', 'label'])"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "(It turns out that GitHub doesn't render the newlines, but I promise they're there!)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "90413c9b8e69458b8c6c3b82ce71ddb9",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Computing probs:   0%|          | 0/500 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "## $0.38\n",
                "choices = (openai.api\n",
                "           .gpt_complete(df['prompt_mc'],\n",
                "                         ask_if_ok=True,\n",
                "                         model='text-davinci-003',\n",
                "                         max_tokens=4, ## need to allow for \"\\n\\nAnswer A\"\n",
                "                         logprobs=1))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": [
                "completions_mc = [choice['text'] for choice in choices]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": [
                "def process_completion(completion: str, class_chars: Sequence[str],\n",
                "                       prefix_remove: str='Answer ', strip_chars: str=' \\n.',\n",
                "                       default=-1) -> int:\n",
                "    if any(len(class_char) != 1 for class_char in class_chars):\n",
                "        raise ValueError('Elements of class_chars must be a single character.')\n",
                "    completion = remove_prefix(completion, prefix_remove)\n",
                "    completion_stripped = completion.strip(strip_chars)\n",
                "    if not completion_stripped:\n",
                "        return default\n",
                "    completion_char_lower = completion_stripped[0].lower()\n",
                "    class_chars_lower = [class_char.lower() for class_char in class_chars]\n",
                "    try:\n",
                "        return class_chars_lower.index(completion_char_lower)\n",
                "    except ValueError:\n",
                "        return default"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [],
            "source": [
                "class_chars = ('A', 'B')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [],
            "source": [
                "pred_classes_cvs = [process_completion(completion, class_chars)\n",
                "                    for completion in completions_mc]"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Check that all of the sampled completions could be mapped to a label 0 or 1:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "1.0"
                        ]
                    },
                    "execution_count": 21,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "(pd.Series(pred_classes_cvs) != -1).mean()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "0.952"
                        ]
                    },
                    "execution_count": 22,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "(pred_classes_cvs == df['label']).mean()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This performance boost is pretty significant, which is a bit of a bummer. Costs are similar, so there isn't a good reason to use this package instead of CVS for COPA."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's see how CVS w/ `text-curie-001` performs. Hypothesis: shouldn't be too bad given the curie result above."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "906cc7ad9d984a4ba1b83d7e83b82a2b",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Computing probs:   0%|          | 0/500 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "## $0.04\n",
                "choices_curie = (openai.api\n",
                "                 .gpt_complete(df['prompt_mc'],\n",
                "                               ask_if_ok=True,\n",
                "                               model='text-curie-001',\n",
                "                               max_tokens=5,\n",
                "                               logprobs=1))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [],
            "source": [
                "completions_mc_curie = [choice['text'] for choice in choices_curie]\n",
                "pred_classes_cvs_curie = [process_completion(completion, class_chars)\n",
                "                          for completion in completions_mc_curie]"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's see how many of these sampled completions are actually \"valid\", i.e., in the label set"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "0.598"
                        ]
                    },
                    "execution_count": 25,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "pred_classes_cvs_curie = pd.Series(pred_classes_cvs_curie, index=df.index)\n",
                "(pred_classes_cvs_curie != -1).mean()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "0.29"
                        ]
                    },
                    "execution_count": 26,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "(pred_classes_cvs_curie == df['label']).mean()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Ouch, much worse than random guessing. Hypothesis very rejected. Let's see how often the valid completions are accurate."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "0.48494983277591974"
                        ]
                    },
                    "execution_count": 27,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "_mask_valid = pred_classes_cvs_curie != -1\n",
                "(pred_classes_cvs_curie[_mask_valid]\n",
                " ==\n",
                " df.loc[_mask_valid, 'label']).mean()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Evaluate CVS (chat)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "How does the chat completion endpoint perform on COPA? I think it makes sense to use the same prompt as above."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "9fab87f5c0d349f290ccd294d7655930",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Completing chats:   0%|          | 0/500 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "## $0.04\n",
                "## can take a while!\n",
                "## idk yet how to batch for the chat endpoint. For correctness, I'll just send\n",
                "## texts 1-by-1\n",
                "choices_chat = (openai.api\n",
                "                .gpt_chat_complete(df['prompt_mc'],\n",
                "                                   ask_if_ok=True,\n",
                "                                   max_tokens=5))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [],
            "source": [
                "completions_chat = pd.Series([choice['message']['content']\n",
                "                              for choice in choices_chat],\n",
                "                             index=df.index)\n",
                "\n",
                "pred_classes_chat = pd.Series([process_completion(completion, class_chars)\n",
                "                               for completion in completions_chat],\n",
                "                              index=df.index)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "As usual, we need to check that completions are valid."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "0.97"
                        ]
                    },
                    "execution_count": 30,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "mask_valid = pred_classes_chat != -1\n",
                "mask_valid.mean()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "What do invalid completions look like?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "32       There is not enough\n",
                            "36           Neither A nor B\n",
                            "47           Neither A nor B\n",
                            "69       There is not enough\n",
                            "83           The answer is B\n",
                            "97           Neither A nor B\n",
                            "145          Neither A nor B\n",
                            "161          Neither A nor B\n",
                            "173    I would classify this\n",
                            "180               I'm sorry,\n",
                            "187          Neither A nor B\n",
                            "210               I'm sorry,\n",
                            "260      There is not enough\n",
                            "312          Neither A nor B\n",
                            "413          Sorry, I cannot\n",
                            "dtype: object"
                        ]
                    },
                    "execution_count": 31,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "completions_chat[~mask_valid]"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "It's uncertain about these. TODO: see if it makes sense"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "What's the accuracy on all completions?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "0.866"
                        ]
                    },
                    "execution_count": 32,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "(pred_classes_chat == df['label']).mean()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "What's the accuracy on *valid* completions?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "0.8927835051546392"
                        ]
                    },
                    "execution_count": 33,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "(pred_classes_chat[mask_valid] == df.loc[mask_valid, 'label']).mean()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Evaluate question"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "There are different ways to format a prompt-completion problem. Since `text-davinci-003` was trained w/ RLHF, it's worth asking whether a more RLHF-type of prompt would work better. Let's see how performance changes by formatting the problem as a question:\n",
                "\n",
                "```\n",
                "The man broke his toe. What was the cause of this? \n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<style type=\"text/css\">\n",
                            "#T_bb506_row0_col0, #T_bb506_row0_col1, #T_bb506_row0_col2, #T_bb506_row0_col3, #T_bb506_row1_col0, #T_bb506_row1_col1, #T_bb506_row1_col2, #T_bb506_row1_col3, #T_bb506_row2_col0, #T_bb506_row2_col1, #T_bb506_row2_col2, #T_bb506_row2_col3 {\n",
                            "  text-align: left;\n",
                            "  white-space: pre-wrap;\n",
                            "}\n",
                            "</style>\n",
                            "<table id=\"T_bb506\">\n",
                            "  <thead>\n",
                            "    <tr>\n",
                            "      <th class=\"blank level0\" >&nbsp;</th>\n",
                            "      <th id=\"T_bb506_level0_col0\" class=\"col_heading level0 col0\" >prompt_question</th>\n",
                            "      <th id=\"T_bb506_level0_col1\" class=\"col_heading level0 col1\" >choice1</th>\n",
                            "      <th id=\"T_bb506_level0_col2\" class=\"col_heading level0 col2\" >choice2</th>\n",
                            "      <th id=\"T_bb506_level0_col3\" class=\"col_heading level0 col3\" >label</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th id=\"T_bb506_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
                            "      <td id=\"T_bb506_row0_col0\" class=\"data row0 col0\" >My body cast a shadow over the grass. What was the cause of this?</td>\n",
                            "      <td id=\"T_bb506_row0_col1\" class=\"data row0 col1\" >The sun was rising.</td>\n",
                            "      <td id=\"T_bb506_row0_col2\" class=\"data row0 col2\" >The grass was cut.</td>\n",
                            "      <td id=\"T_bb506_row0_col3\" class=\"data row0 col3\" >0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th id=\"T_bb506_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
                            "      <td id=\"T_bb506_row1_col0\" class=\"data row1 col0\" >The woman tolerated her friend's difficult behavior. What was the cause of this?</td>\n",
                            "      <td id=\"T_bb506_row1_col1\" class=\"data row1 col1\" >The woman knew her friend was going through a hard time.</td>\n",
                            "      <td id=\"T_bb506_row1_col2\" class=\"data row1 col2\" >The woman felt that her friend took advantage of her kindness.</td>\n",
                            "      <td id=\"T_bb506_row1_col3\" class=\"data row1 col3\" >0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th id=\"T_bb506_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
                            "      <td id=\"T_bb506_row2_col0\" class=\"data row2 col0\" >The women met for coffee. What was the cause of this?</td>\n",
                            "      <td id=\"T_bb506_row2_col1\" class=\"data row2 col1\" >The cafe reopened in a new location.</td>\n",
                            "      <td id=\"T_bb506_row2_col2\" class=\"data row2 col2\" >They wanted to catch up with each other.</td>\n",
                            "      <td id=\"T_bb506_row2_col3\" class=\"data row2 col3\" >1</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n"
                        ],
                        "text/plain": [
                            "<pandas.io.formats.style.Styler at 0x1c0d3723250>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "def prompt_question(premise: str, question: Literal['cause', 'effect']):\n",
                "    if question == 'cause':\n",
                "        question_ = 'What was the cause of this?'\n",
                "    elif question == 'effect':\n",
                "        question_ = 'What happened as a result?'\n",
                "    else:\n",
                "        raise ValueError( \"question must be 'cause' or 'effect'. Got \"\n",
                "                         f'{question}.')\n",
                "    return f'{premise} {question_}'\n",
                "\n",
                "\n",
                "df['prompt_question'] = [prompt_question(premise, question)\n",
                "                         for premise, question\n",
                "                         in zip(df['premise'], df['question'])]\n",
                "\n",
                "\n",
                "display_df(df, columns=['prompt_question', 'choice1', 'choice2', 'label'])"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "According to the [docs](https://platform.openai.com/docs/guides/fine-tuning/data-formatting), best practice is to separate prompts and completions using this string:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'\\n\\n###\\n\\n'"
                        ]
                    },
                    "execution_count": 35,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "openai.api.end_of_prompt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {},
            "outputs": [],
            "source": [
                "examples_question = [Example(prompt=record['prompt_question'],\n",
                "                             completions=(record['choice1'],\n",
                "                                          record['choice2']),\n",
                "                             prior=None,\n",
                "                             end_of_prompt=openai.api.end_of_prompt)\n",
                "                     for record in df.to_dict('records')]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "5a4c46a49928495c96d2b3aee1c30e55",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Computing probs:   0%|          | 0/1000 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "## $0.51\n",
                "pred_probs_question = (openai.classify\n",
                "                       .predict_proba_examples(examples_question,\n",
                "                                               model='text-davinci-003',\n",
                "                                               ask_if_ok=True))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "0.828"
                        ]
                    },
                    "execution_count": 38,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "(pred_probs_question.argmax(axis=1) == df['label']).mean()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Evaluate single-token"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's see how the single-token transformation performs for COPA. Based on the [Evaluate CVS](#evaluate-cvs) result, my hypothesis is that it'll perform slightly better than the [multi-token approach](#run-model). I wouldn't be bummed if it performed better. B/c if I could control the backend, there's still a usability and computational benefit to the idea of returning probabilities for A and B instead of sampling from all possible token sequences."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {},
            "outputs": [],
            "source": [
                "examples_mc = [Example(prompt=record['prompt_mc'],\n",
                "                       completions=('A', 'B'),\n",
                "                       prior=None,\n",
                "                       end_of_prompt=openai.api.end_of_prompt)\n",
                "               for record in df.to_dict('records')]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "249090a6fcb14af8bb7a695d542a68d1",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Computing probs:   0%|          | 0/1000 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "## $0.79\n",
                "## If I could control the backend, the cost would be $0.79/2 = $0.385\n",
                "pred_probs_mc = (openai.classify\n",
                "                 .predict_proba_examples(examples_mc,\n",
                "                                         model='text-davinci-003',\n",
                "                                         ask_if_ok=True))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "0.934"
                        ]
                    },
                    "execution_count": 41,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "(pred_probs_mc.argmax(axis=1) == df['label']).mean()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Conclusion: performs 1-2% better than the [multi-token approach](#run-model). But it's consistently 1-3% worse than [CVS](#evaluate-cvs), which is difficult to explain. The only explanations are:\n",
                "  1. It's better to allow a more free-form, sampled output, which you then parse to get a classification.\n",
                "  2. API clients aren't allowed to use the `<|endoftext|>` token to separate prompt from completion, which could be impactful b/c that was used during training.\n",
                "\n",
                "It also costs twice as much. But that's an artifact of the way the endpoint works. For prompts like this, it takes 1 `model()` call to give us the data we need: the probability distribution of (single tokens) `'A'` and `'B'` conditional on the prompt."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "lmc",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.16"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "68daa88f78f5c448099edb3a6d3dee27486a6add8824ae1cbe4c903ef8faec70"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
